{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Alekseyl\\Envs\\absa\\lib\\site-packages\\gensim\\utils.py:855: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "from importlib import reload\n",
    "from utils import *\n",
    "import test\n",
    "import polarity\n",
    "import acd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading real w2v...\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "w2v = load_w2v()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- ACD:\n",
      "Loading tokenizer...\n",
      "Loading stemmer...\n",
      "Loading dataset...\n",
      "Training...\n",
      "Evaluating...\n",
      "F1: {'f1': 0.6904231625835189, 'precision': 0.7029478458049887, 'recall': 0.6783369803063457, 'tp': 310, 'fp': 131, 'fn': 147}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Alekseyl\\Envs\\absa\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:563: ConvergenceWarning: Stochastic Optimizer: Maximum iterations reached and the optimization hasn't converged yet.\n",
      "  % (), ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "reload(acd)\n",
    "acd_ = acd.ACD(w2v)\n",
    "# acd_.train_acd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- PD:\n",
      "Loading tokenizer...\n",
      "Loading stemmer...\n",
      "Loading parser...\n",
      "Loading dataset...\n",
      "Core NLP Parser preprocessing result pickled\n",
      "get_pd_ds progress: 0/1708\n",
      "get_pd_ds progress: 100/1708\n",
      "get_pd_ds progress: 200/1708\n",
      "get_pd_ds progress: 300/1708\n",
      "get_pd_ds progress: 400/1708\n",
      "get_pd_ds progress: 500/1708\n",
      "get_pd_ds progress: 600/1708\n",
      "get_pd_ds progress: 700/1708\n",
      "get_pd_ds progress: 800/1708\n",
      "get_pd_ds progress: 900/1708\n",
      "get_pd_ds progress: 1000/1708\n",
      "get_pd_ds progress: 1100/1708\n",
      "get_pd_ds progress: 1200/1708\n",
      "get_pd_ds progress: 1300/1708\n",
      "get_pd_ds progress: 1400/1708\n",
      "get_pd_ds progress: 1500/1708\n",
      "get_pd_ds progress: 1600/1708\n",
      "get_pd_ds progress: 1700/1708\n",
      "Train on 2005 samples, validate on 502 samples\n",
      "Epoch 1/100\n",
      "2005/2005 [==============================] - 1s - loss: 1.0321 - acc: 0.7137 - val_loss: 0.9958 - val_acc: 0.4243\n",
      "Epoch 2/100\n",
      "2005/2005 [==============================] - 0s - loss: 0.9368 - acc: 0.7202 - val_loss: 0.9699 - val_acc: 0.4243\n",
      "Epoch 3/100\n",
      "2005/2005 [==============================] - 0s - loss: 0.8940 - acc: 0.7202 - val_loss: 0.9719 - val_acc: 0.4243\n",
      "Epoch 4/100\n",
      "2005/2005 [==============================] - 0s - loss: 0.8843 - acc: 0.7202 - val_loss: 0.9724 - val_acc: 0.4243\n",
      "Epoch 5/100\n",
      "2005/2005 [==============================] - 0s - loss: 0.8827 - acc: 0.7202 - val_loss: 0.9693 - val_acc: 0.4243\n",
      "Epoch 6/100\n",
      "2005/2005 [==============================] - 0s - loss: 0.8844 - acc: 0.7202 - val_loss: 0.9687 - val_acc: 0.4243\n",
      "Epoch 7/100\n",
      "2005/2005 [==============================] - 0s - loss: 0.8757 - acc: 0.7202 - val_loss: 0.9717 - val_acc: 0.4243\n",
      "Epoch 8/100\n",
      "2005/2005 [==============================] - 0s - loss: 0.8700 - acc: 0.7202 - val_loss: 0.9693 - val_acc: 0.4243\n",
      "Epoch 9/100\n",
      "2005/2005 [==============================] - 0s - loss: 0.8524 - acc: 0.7202 - val_loss: 0.9711 - val_acc: 0.4243\n",
      "Epoch 10/100\n",
      "2005/2005 [==============================] - 0s - loss: 0.8371 - acc: 0.7202 - val_loss: 0.9716 - val_acc: 0.4243\n",
      "Epoch 11/100\n",
      "2005/2005 [==============================] - 0s - loss: 0.8446 - acc: 0.7202 - val_loss: 0.9739 - val_acc: 0.4243\n",
      "Epoch 12/100\n",
      "2005/2005 [==============================] - 0s - loss: 0.8349 - acc: 0.7202 - val_loss: 0.9759 - val_acc: 0.4243\n",
      "Epoch 13/100\n",
      "2005/2005 [==============================] - 1s - loss: 0.8376 - acc: 0.7202 - val_loss: 0.9767 - val_acc: 0.4243\n",
      "Epoch 14/100\n",
      "2005/2005 [==============================] - 0s - loss: 0.8159 - acc: 0.7202 - val_loss: 0.9782 - val_acc: 0.4243\n",
      "Epoch 15/100\n",
      "2005/2005 [==============================] - 0s - loss: 0.8186 - acc: 0.7202 - val_loss: 0.9790 - val_acc: 0.4243\n",
      "Epoch 16/100\n",
      "2005/2005 [==============================] - 0s - loss: 0.8111 - acc: 0.7202 - val_loss: 0.9810 - val_acc: 0.4243\n",
      "Epoch 17/100\n",
      "2005/2005 [==============================] - 1s - loss: 0.8026 - acc: 0.7202 - val_loss: 0.9827 - val_acc: 0.4243\n",
      "Epoch 18/100\n",
      "2005/2005 [==============================] - 0s - loss: 0.7935 - acc: 0.7202 - val_loss: 0.9824 - val_acc: 0.4243\n",
      "Epoch 19/100\n",
      "2005/2005 [==============================] - 0s - loss: 0.7880 - acc: 0.7202 - val_loss: 0.9843 - val_acc: 0.4243\n",
      "Epoch 20/100\n",
      "2005/2005 [==============================] - 0s - loss: 0.8007 - acc: 0.7202 - val_loss: 0.9832 - val_acc: 0.4243\n",
      "Epoch 21/100\n",
      "2005/2005 [==============================] - 0s - loss: 0.7814 - acc: 0.7202 - val_loss: 0.9846 - val_acc: 0.4243\n",
      "Epoch 22/100\n",
      "2005/2005 [==============================] - 0s - loss: 0.7795 - acc: 0.7202 - val_loss: 0.9850 - val_acc: 0.4243\n",
      "Epoch 23/100\n",
      "2005/2005 [==============================] - 0s - loss: 0.7672 - acc: 0.7202 - val_loss: 0.9880 - val_acc: 0.4243\n",
      "Epoch 24/100\n",
      "2005/2005 [==============================] - 1s - loss: 0.7667 - acc: 0.7202 - val_loss: 0.9899 - val_acc: 0.4243\n",
      "Epoch 25/100\n",
      "2005/2005 [==============================] - 0s - loss: 0.7631 - acc: 0.7202 - val_loss: 0.9898 - val_acc: 0.4243\n",
      "Epoch 26/100\n",
      "2005/2005 [==============================] - 0s - loss: 0.7669 - acc: 0.7202 - val_loss: 0.9905 - val_acc: 0.4243\n",
      "Epoch 27/100\n",
      "2005/2005 [==============================] - 0s - loss: 0.7644 - acc: 0.7202 - val_loss: 0.9902 - val_acc: 0.4243\n",
      "Epoch 28/100\n",
      "2005/2005 [==============================] - 0s - loss: 0.7530 - acc: 0.7202 - val_loss: 0.9921 - val_acc: 0.4243\n",
      "Epoch 29/100\n",
      "2005/2005 [==============================] - 0s - loss: 0.7530 - acc: 0.7202 - val_loss: 0.9934 - val_acc: 0.4243\n",
      "Epoch 30/100\n",
      "2005/2005 [==============================] - 0s - loss: 0.7531 - acc: 0.7202 - val_loss: 0.9944 - val_acc: 0.4243\n",
      "Epoch 31/100\n",
      "2005/2005 [==============================] - 0s - loss: 0.7376 - acc: 0.7202 - val_loss: 0.9974 - val_acc: 0.4243\n",
      "Epoch 32/100\n",
      "2005/2005 [==============================] - 0s - loss: 0.7365 - acc: 0.7202 - val_loss: 0.9988 - val_acc: 0.4243\n",
      "Epoch 33/100\n",
      "2005/2005 [==============================] - 0s - loss: 0.7390 - acc: 0.7202 - val_loss: 0.9998 - val_acc: 0.4243\n",
      "Epoch 34/100\n",
      "2005/2005 [==============================] - 0s - loss: 0.7326 - acc: 0.7202 - val_loss: 1.0008 - val_acc: 0.4243\n",
      "Epoch 35/100\n",
      "2005/2005 [==============================] - 0s - loss: 0.7278 - acc: 0.7202 - val_loss: 1.0023 - val_acc: 0.4243\n",
      "Epoch 36/100\n",
      "2005/2005 [==============================] - 0s - loss: 0.7257 - acc: 0.7202 - val_loss: 1.0040 - val_acc: 0.4243\n",
      "Epoch 37/100\n",
      "2005/2005 [==============================] - 0s - loss: 0.7290 - acc: 0.7202 - val_loss: 1.0037 - val_acc: 0.4243\n",
      "Epoch 38/100\n",
      "2005/2005 [==============================] - 0s - loss: 0.7263 - acc: 0.7202 - val_loss: 1.0042 - val_acc: 0.4243\n",
      "Epoch 39/100\n",
      "2005/2005 [==============================] - 0s - loss: 0.7195 - acc: 0.7202 - val_loss: 1.0066 - val_acc: 0.4243\n",
      "Epoch 40/100\n",
      "2005/2005 [==============================] - 0s - loss: 0.7158 - acc: 0.7202 - val_loss: 1.0070 - val_acc: 0.4243\n",
      "Epoch 41/100\n",
      "2005/2005 [==============================] - 0s - loss: 0.7181 - acc: 0.7202 - val_loss: 1.0064 - val_acc: 0.4243\n",
      "Epoch 42/100\n",
      "2005/2005 [==============================] - 1s - loss: 0.7114 - acc: 0.7202 - val_loss: 1.0067 - val_acc: 0.4243\n",
      "Epoch 43/100\n",
      "2005/2005 [==============================] - 1s - loss: 0.7111 - acc: 0.7202 - val_loss: 1.0082 - val_acc: 0.4243\n",
      "Epoch 44/100\n",
      "2005/2005 [==============================] - 1s - loss: 0.7086 - acc: 0.7202 - val_loss: 1.0095 - val_acc: 0.4243\n",
      "Epoch 45/100\n",
      "2005/2005 [==============================] - 1s - loss: 0.7047 - acc: 0.7202 - val_loss: 1.0094 - val_acc: 0.4243\n",
      "Epoch 46/100\n",
      "2005/2005 [==============================] - 0s - loss: 0.7059 - acc: 0.7202 - val_loss: 1.0101 - val_acc: 0.4243\n",
      "Epoch 47/100\n",
      "2005/2005 [==============================] - 0s - loss: 0.7094 - acc: 0.7202 - val_loss: 1.0119 - val_acc: 0.4243\n",
      "Epoch 48/100\n",
      "2005/2005 [==============================] - 1s - loss: 0.6954 - acc: 0.7202 - val_loss: 1.0136 - val_acc: 0.4243\n",
      "Epoch 49/100\n",
      "2005/2005 [==============================] - 0s - loss: 0.7028 - acc: 0.7202 - val_loss: 1.0131 - val_acc: 0.4243\n",
      "Epoch 50/100\n",
      "2005/2005 [==============================] - 0s - loss: 0.7050 - acc: 0.7202 - val_loss: 1.0108 - val_acc: 0.4243\n",
      "Epoch 51/100\n",
      "2005/2005 [==============================] - 0s - loss: 0.6992 - acc: 0.7202 - val_loss: 1.0120 - val_acc: 0.4243\n",
      "Epoch 52/100\n",
      "2005/2005 [==============================] - 0s - loss: 0.6997 - acc: 0.7202 - val_loss: 1.0119 - val_acc: 0.4243\n",
      "Epoch 53/100\n",
      "2005/2005 [==============================] - 0s - loss: 0.6959 - acc: 0.7202 - val_loss: 1.0114 - val_acc: 0.4243\n",
      "Epoch 54/100\n",
      "  50/2005 [..............................] - ETA: 0s - loss: 0.6607 - acc: 0.7600"
     ]
    }
   ],
   "source": [
    "import polarity\n",
    "reload(polarity)\n",
    "# pd = polarity.PD(W2VMock(), acd_)\n",
    "pd = polarity.PD(w2v, acd_)\n",
    "pd.train_pd_keras()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "absa",
   "language": "python",
   "name": "absa"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
