{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Alekseyl\\Envs\\absa\\lib\\site-packages\\gensim\\utils.py:855: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from importlib import reload\n",
    "from utils import *\n",
    "import test\n",
    "import polarity\n",
    "import acd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading real w2v...\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "w2v = load_w2v()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- ACD:\n",
      "Loading tokenizer...\n",
      "Loading stemmer...\n",
      "Loading dataset...\n",
      "Training...\n",
      "Evaluating...\n",
      "F1: {'f1': 0.672645739910314, 'precision': 0.6896551724137931, 'recall': 0.6564551422319475, 'tp': 300, 'fp': 135, 'fn': 157}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Alekseyl\\Envs\\absa\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:563: ConvergenceWarning: Stochastic Optimizer: Maximum iterations reached and the optimization hasn't converged yet.\n",
      "  % (), ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "reload(acd)\n",
    "acd_ = acd.ACD(w2v)\n",
    "acd_.train_acd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- PD:\n",
      "Loading tokenizer...\n",
      "Loading stemmer...\n",
      "Loading parser...\n",
      "Loading dataset...\n",
      "Core NLP Parser preprocessing result pickled\n",
      "get_pd_ds progress: 0/1708\n",
      "get_pd_ds progress: 100/1708\n",
      "get_pd_ds progress: 200/1708\n",
      "get_pd_ds progress: 300/1708\n",
      "get_pd_ds progress: 400/1708\n",
      "get_pd_ds progress: 500/1708\n",
      "get_pd_ds progress: 600/1708\n",
      "get_pd_ds progress: 700/1708\n",
      "get_pd_ds progress: 800/1708\n",
      "get_pd_ds progress: 900/1708\n",
      "get_pd_ds progress: 1000/1708\n",
      "get_pd_ds progress: 1100/1708\n",
      "get_pd_ds progress: 1200/1708\n",
      "get_pd_ds progress: 1300/1708\n",
      "get_pd_ds progress: 1400/1708\n",
      "get_pd_ds progress: 1500/1708\n",
      "get_pd_ds progress: 1600/1708\n",
      "get_pd_ds progress: 1700/1708\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_2 (InputLayer)             (None, 30, 300)       0                                            \n",
      "____________________________________________________________________________________________________\n",
      "conv1d_4 (Conv1D)                (None, 28, 20)        18020                                        \n",
      "____________________________________________________________________________________________________\n",
      "conv1d_5 (Conv1D)                (None, 27, 20)        24020                                        \n",
      "____________________________________________________________________________________________________\n",
      "conv1d_6 (Conv1D)                (None, 26, 20)        30020                                        \n",
      "____________________________________________________________________________________________________\n",
      "global_max_pooling1d_4 (GlobalMa (None, 20)            0                                            \n",
      "____________________________________________________________________________________________________\n",
      "global_max_pooling1d_5 (GlobalMa (None, 20)            0                                            \n",
      "____________________________________________________________________________________________________\n",
      "global_max_pooling1d_6 (GlobalMa (None, 20)            0                                            \n",
      "____________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)      (None, 60)            0                                            \n",
      "____________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)              (None, 60)            0                                            \n",
      "____________________________________________________________________________________________________\n",
      "dense_2 (Dense)                  (None, 3)             183                                          \n",
      "====================================================================================================\n",
      "Total params: 72,243\n",
      "Trainable params: 72,243\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n",
      "None\n",
      "Train on 2005 samples, validate on 502 samples\n",
      "Epoch 1/100\n",
      "2005/2005 [==============================] - 1s - loss: 0.8790 - acc: 0.7032 - val_loss: 1.0087 - val_acc: 0.4243\n",
      "Epoch 2/100\n",
      "2005/2005 [==============================] - 1s - loss: 0.7069 - acc: 0.7202 - val_loss: 0.9455 - val_acc: 0.4243\n",
      "Epoch 3/100\n",
      "2005/2005 [==============================] - 1s - loss: 0.6458 - acc: 0.7252 - val_loss: 0.8801 - val_acc: 0.4243\n",
      "Epoch 4/100\n",
      "2005/2005 [==============================] - 1s - loss: 0.6039 - acc: 0.7372 - val_loss: 0.7837 - val_acc: 0.5239\n",
      "Epoch 5/100\n",
      "2005/2005 [==============================] - 1s - loss: 0.5652 - acc: 0.7636 - val_loss: 0.8008 - val_acc: 0.5060\n",
      "Epoch 6/100\n",
      "2005/2005 [==============================] - 1s - loss: 0.5424 - acc: 0.7845 - val_loss: 0.7864 - val_acc: 0.5498\n",
      "Epoch 7/100\n",
      "2005/2005 [==============================] - 1s - loss: 0.5085 - acc: 0.8025 - val_loss: 0.7292 - val_acc: 0.6195\n",
      "Epoch 8/100\n",
      "2005/2005 [==============================] - 1s - loss: 0.4847 - acc: 0.8234 - val_loss: 0.6833 - val_acc: 0.6693\n",
      "Epoch 9/100\n",
      "2005/2005 [==============================] - 1s - loss: 0.4659 - acc: 0.8314 - val_loss: 0.7186 - val_acc: 0.6315\n",
      "Epoch 10/100\n",
      "2005/2005 [==============================] - 1s - loss: 0.4454 - acc: 0.8404 - val_loss: 0.6914 - val_acc: 0.6494\n",
      "Epoch 11/100\n",
      "2005/2005 [==============================] - 1s - loss: 0.4307 - acc: 0.8419 - val_loss: 0.6624 - val_acc: 0.6972\n",
      "Epoch 12/100\n",
      "2005/2005 [==============================] - 1s - loss: 0.4150 - acc: 0.8599 - val_loss: 0.6881 - val_acc: 0.6534\n",
      "Epoch 13/100\n",
      "2005/2005 [==============================] - 1s - loss: 0.4095 - acc: 0.8608 - val_loss: 0.6831 - val_acc: 0.6653\n",
      "Epoch 14/100\n",
      "2005/2005 [==============================] - 1s - loss: 0.3935 - acc: 0.8628 - val_loss: 0.7025 - val_acc: 0.6474\n",
      "Epoch 15/100\n",
      "2005/2005 [==============================] - 1s - loss: 0.3918 - acc: 0.8608 - val_loss: 0.6590 - val_acc: 0.6892\n",
      "Epoch 16/100\n",
      "2005/2005 [==============================] - 1s - loss: 0.3772 - acc: 0.8758 - val_loss: 0.6404 - val_acc: 0.7191\n",
      "Epoch 17/100\n",
      "2005/2005 [==============================] - 1s - loss: 0.3801 - acc: 0.8673 - val_loss: 0.7283 - val_acc: 0.6394\n",
      "Epoch 18/100\n",
      "2005/2005 [==============================] - 1s - loss: 0.3593 - acc: 0.8773 - val_loss: 0.6344 - val_acc: 0.7271\n",
      "Epoch 19/100\n",
      "2005/2005 [==============================] - 1s - loss: 0.3622 - acc: 0.8763 - val_loss: 0.6499 - val_acc: 0.7012\n",
      "Epoch 20/100\n",
      "2005/2005 [==============================] - 1s - loss: 0.3583 - acc: 0.8758 - val_loss: 0.6456 - val_acc: 0.7112\n",
      "Epoch 21/100\n",
      "2005/2005 [==============================] - 1s - loss: 0.3511 - acc: 0.8853 - val_loss: 0.6538 - val_acc: 0.6952\n",
      "Epoch 22/100\n",
      "2005/2005 [==============================] - 1s - loss: 0.3369 - acc: 0.8858 - val_loss: 0.6054 - val_acc: 0.7809\n",
      "Epoch 23/100\n",
      "2005/2005 [==============================] - 1s - loss: 0.3414 - acc: 0.8843 - val_loss: 0.6260 - val_acc: 0.7430\n",
      "Epoch 24/100\n",
      "2005/2005 [==============================] - 1s - loss: 0.3261 - acc: 0.8903 - val_loss: 0.6274 - val_acc: 0.7410\n",
      "Epoch 25/100\n",
      "2005/2005 [==============================] - 1s - loss: 0.3295 - acc: 0.8868 - val_loss: 0.6576 - val_acc: 0.7052\n",
      "Epoch 26/100\n",
      "2005/2005 [==============================] - 1s - loss: 0.3278 - acc: 0.8818 - val_loss: 0.6374 - val_acc: 0.7291\n",
      "Epoch 27/100\n",
      "2005/2005 [==============================] - 1s - loss: 0.3153 - acc: 0.8878 - val_loss: 0.6684 - val_acc: 0.6873\n",
      "Epoch 28/100\n",
      "2005/2005 [==============================] - 1s - loss: 0.3147 - acc: 0.8943 - val_loss: 0.6253 - val_acc: 0.7430\n",
      "Epoch 29/100\n",
      "2005/2005 [==============================] - 1s - loss: 0.3155 - acc: 0.8918 - val_loss: 0.6619 - val_acc: 0.6932\n",
      "Epoch 30/100\n",
      "2005/2005 [==============================] - 1s - loss: 0.3041 - acc: 0.8933 - val_loss: 0.6111 - val_acc: 0.7610\n",
      "Epoch 31/100\n",
      "2005/2005 [==============================] - 1s - loss: 0.2942 - acc: 0.9007 - val_loss: 0.6378 - val_acc: 0.7311\n",
      "Epoch 32/100\n",
      "2005/2005 [==============================] - 1s - loss: 0.3017 - acc: 0.8938 - val_loss: 0.6109 - val_acc: 0.7629\n",
      "Epoch 33/100\n",
      "2005/2005 [==============================] - 1s - loss: 0.2940 - acc: 0.8958 - val_loss: 0.5953 - val_acc: 0.7769\n",
      "Epoch 34/100\n",
      "2005/2005 [==============================] - 1s - loss: 0.2912 - acc: 0.8988 - val_loss: 0.6781 - val_acc: 0.6892\n",
      "Epoch 35/100\n",
      "2005/2005 [==============================] - 1s - loss: 0.2917 - acc: 0.8983 - val_loss: 0.6449 - val_acc: 0.7251\n",
      "Epoch 36/100\n",
      "2005/2005 [==============================] - 1s - loss: 0.2917 - acc: 0.8983 - val_loss: 0.6125 - val_acc: 0.7610\n",
      "Epoch 37/100\n",
      "2005/2005 [==============================] - 1s - loss: 0.2876 - acc: 0.8973 - val_loss: 0.6184 - val_acc: 0.7570\n",
      "Epoch 38/100\n",
      "2005/2005 [==============================] - 1s - loss: 0.2708 - acc: 0.9107 - val_loss: 0.6828 - val_acc: 0.6873\n",
      "Epoch 39/100\n",
      "2005/2005 [==============================] - 1s - loss: 0.2732 - acc: 0.9067 - val_loss: 0.6156 - val_acc: 0.7590\n",
      "Epoch 40/100\n",
      "2005/2005 [==============================] - 1s - loss: 0.2725 - acc: 0.9097 - val_loss: 0.6228 - val_acc: 0.7530\n",
      "Epoch 41/100\n",
      "2005/2005 [==============================] - 1s - loss: 0.2732 - acc: 0.9052 - val_loss: 0.6654 - val_acc: 0.7072\n",
      "Epoch 42/100\n",
      "2005/2005 [==============================] - 1s - loss: 0.2624 - acc: 0.9082 - val_loss: 0.6652 - val_acc: 0.7052\n",
      "Epoch 43/100\n",
      "2005/2005 [==============================] - 1s - loss: 0.2583 - acc: 0.9137 - val_loss: 0.6256 - val_acc: 0.7450\n",
      "Epoch 44/100\n",
      "2005/2005 [==============================] - 1s - loss: 0.2585 - acc: 0.9077 - val_loss: 0.6690 - val_acc: 0.6992\n",
      "Epoch 45/100\n",
      "2005/2005 [==============================] - 1s - loss: 0.2502 - acc: 0.9122 - val_loss: 0.6555 - val_acc: 0.7191\n",
      "Epoch 46/100\n",
      "2005/2005 [==============================] - 1s - loss: 0.2614 - acc: 0.9052 - val_loss: 0.6374 - val_acc: 0.7410\n",
      "Epoch 47/100\n",
      "2005/2005 [==============================] - 1s - loss: 0.2627 - acc: 0.9072 - val_loss: 0.6437 - val_acc: 0.7331\n",
      "Epoch 48/100\n",
      "2005/2005 [==============================] - 1s - loss: 0.2532 - acc: 0.9092 - val_loss: 0.6536 - val_acc: 0.7271\n",
      "Epoch 49/100\n",
      "2005/2005 [==============================] - 1s - loss: 0.2431 - acc: 0.9197 - val_loss: 0.6650 - val_acc: 0.7171\n",
      "Epoch 50/100\n",
      "2005/2005 [==============================] - 1s - loss: 0.2388 - acc: 0.9177 - val_loss: 0.6162 - val_acc: 0.7530\n",
      "Epoch 51/100\n",
      "2005/2005 [==============================] - 1s - loss: 0.2417 - acc: 0.9192 - val_loss: 0.6398 - val_acc: 0.7351\n",
      "Epoch 52/100\n",
      "2005/2005 [==============================] - 1s - loss: 0.2393 - acc: 0.9127 - val_loss: 0.6229 - val_acc: 0.7530\n",
      "Epoch 53/100\n",
      "2005/2005 [==============================] - 1s - loss: 0.2325 - acc: 0.9162 - val_loss: 0.6493 - val_acc: 0.7331\n",
      "Epoch 54/100\n",
      "2005/2005 [==============================] - 1s - loss: 0.2352 - acc: 0.9167 - val_loss: 0.6076 - val_acc: 0.7689\n",
      "Epoch 55/100\n",
      "2005/2005 [==============================] - 1s - loss: 0.2383 - acc: 0.9157 - val_loss: 0.6426 - val_acc: 0.7351\n",
      "Epoch 56/100\n",
      "2005/2005 [==============================] - 1s - loss: 0.2300 - acc: 0.9182 - val_loss: 0.6531 - val_acc: 0.7291\n",
      "Epoch 57/100\n",
      "2005/2005 [==============================] - 1s - loss: 0.2229 - acc: 0.9192 - val_loss: 0.6450 - val_acc: 0.7331\n",
      "Epoch 58/100\n",
      "2005/2005 [==============================] - 1s - loss: 0.2261 - acc: 0.9187 - val_loss: 0.6422 - val_acc: 0.7351\n",
      "Epoch 59/100\n",
      "2005/2005 [==============================] - 1s - loss: 0.2235 - acc: 0.9247 - val_loss: 0.6487 - val_acc: 0.7331\n",
      "Epoch 60/100\n",
      "2005/2005 [==============================] - 1s - loss: 0.2183 - acc: 0.9202 - val_loss: 0.6731 - val_acc: 0.7271\n",
      "Epoch 61/100\n",
      "2005/2005 [==============================] - 1s - loss: 0.2254 - acc: 0.9162 - val_loss: 0.6180 - val_acc: 0.7570\n",
      "Epoch 62/100\n",
      "2005/2005 [==============================] - 1s - loss: 0.2193 - acc: 0.9187 - val_loss: 0.6357 - val_acc: 0.7450\n",
      "Epoch 63/100\n",
      "2005/2005 [==============================] - 1s - loss: 0.2177 - acc: 0.9217 - val_loss: 0.6319 - val_acc: 0.7450\n",
      "Epoch 64/100\n",
      "2005/2005 [==============================] - 1s - loss: 0.2149 - acc: 0.9197 - val_loss: 0.6421 - val_acc: 0.7331\n",
      "Epoch 65/100\n",
      "2005/2005 [==============================] - 1s - loss: 0.2107 - acc: 0.9227 - val_loss: 0.6757 - val_acc: 0.7191\n",
      "Epoch 66/100\n",
      "2005/2005 [==============================] - 1s - loss: 0.2062 - acc: 0.9257 - val_loss: 0.6291 - val_acc: 0.7390\n",
      "Epoch 67/100\n",
      "2005/2005 [==============================] - 1s - loss: 0.2103 - acc: 0.9257 - val_loss: 0.6615 - val_acc: 0.7251\n",
      "Epoch 68/100\n",
      "2005/2005 [==============================] - 1s - loss: 0.2070 - acc: 0.9257 - val_loss: 0.6930 - val_acc: 0.7171\n",
      "Epoch 69/100\n",
      "2005/2005 [==============================] - 1s - loss: 0.2044 - acc: 0.9262 - val_loss: 0.6213 - val_acc: 0.7570\n",
      "Epoch 70/100\n",
      "2005/2005 [==============================] - 1s - loss: 0.1996 - acc: 0.9307 - val_loss: 0.6213 - val_acc: 0.7590\n",
      "Epoch 71/100\n",
      "2005/2005 [==============================] - 1s - loss: 0.2020 - acc: 0.9217 - val_loss: 0.6306 - val_acc: 0.7490\n",
      "Epoch 72/100\n",
      "2005/2005 [==============================] - 1s - loss: 0.2070 - acc: 0.9247 - val_loss: 0.6354 - val_acc: 0.7430\n",
      "Epoch 73/100\n",
      "2005/2005 [==============================] - 1s - loss: 0.1947 - acc: 0.9272 - val_loss: 0.6221 - val_acc: 0.7590\n",
      "Epoch 74/100\n",
      "2005/2005 [==============================] - 1s - loss: 0.2034 - acc: 0.9292 - val_loss: 0.6588 - val_acc: 0.7291\n",
      "Epoch 75/100\n",
      "2005/2005 [==============================] - 1s - loss: 0.1930 - acc: 0.9257 - val_loss: 0.6752 - val_acc: 0.7211\n",
      "Epoch 76/100\n",
      "2005/2005 [==============================] - 1s - loss: 0.1913 - acc: 0.9337 - val_loss: 0.6491 - val_acc: 0.7351\n",
      "Epoch 77/100\n",
      "2005/2005 [==============================] - 1s - loss: 0.1923 - acc: 0.9332 - val_loss: 0.6361 - val_acc: 0.7430\n",
      "Epoch 78/100\n",
      "2005/2005 [==============================] - 1s - loss: 0.1893 - acc: 0.9302 - val_loss: 0.6281 - val_acc: 0.7530\n",
      "Epoch 79/100\n",
      "2005/2005 [==============================] - 1s - loss: 0.1864 - acc: 0.9362 - val_loss: 0.6316 - val_acc: 0.7470\n",
      "Epoch 80/100\n",
      "2005/2005 [==============================] - 1s - loss: 0.1834 - acc: 0.9337 - val_loss: 0.6717 - val_acc: 0.7231\n",
      "Epoch 81/100\n",
      "2005/2005 [==============================] - 1s - loss: 0.1805 - acc: 0.9327 - val_loss: 0.6655 - val_acc: 0.7231\n",
      "Epoch 82/100\n",
      "2005/2005 [==============================] - 1s - loss: 0.1738 - acc: 0.9411 - val_loss: 0.6352 - val_acc: 0.7530\n",
      "Epoch 83/100\n",
      "2005/2005 [==============================] - 1s - loss: 0.1840 - acc: 0.9307 - val_loss: 0.6148 - val_acc: 0.7709\n",
      "Epoch 84/100\n",
      "2005/2005 [==============================] - 1s - loss: 0.1792 - acc: 0.9307 - val_loss: 0.6071 - val_acc: 0.7729\n",
      "Epoch 85/100\n",
      "2005/2005 [==============================] - 1s - loss: 0.1851 - acc: 0.9322 - val_loss: 0.6125 - val_acc: 0.7610\n",
      "Epoch 86/100\n",
      "2005/2005 [==============================] - 1s - loss: 0.1824 - acc: 0.9297 - val_loss: 0.6758 - val_acc: 0.7231\n",
      "Epoch 87/100\n",
      "2005/2005 [==============================] - 1s - loss: 0.1790 - acc: 0.9282 - val_loss: 0.6275 - val_acc: 0.7530\n",
      "Epoch 88/100\n",
      "2005/2005 [==============================] - 1s - loss: 0.1735 - acc: 0.9382 - val_loss: 0.6775 - val_acc: 0.7231\n",
      "Epoch 89/100\n",
      "2005/2005 [==============================] - 1s - loss: 0.1710 - acc: 0.9347 - val_loss: 0.6183 - val_acc: 0.7689\n",
      "Epoch 90/100\n",
      "2005/2005 [==============================] - 1s - loss: 0.1786 - acc: 0.9357 - val_loss: 0.6613 - val_acc: 0.7291\n",
      "Epoch 91/100\n",
      "2005/2005 [==============================] - 1s - loss: 0.1746 - acc: 0.9357 - val_loss: 0.6490 - val_acc: 0.7410\n",
      "Epoch 92/100\n",
      "2005/2005 [==============================] - 1s - loss: 0.1722 - acc: 0.9411 - val_loss: 0.6615 - val_acc: 0.7351\n",
      "Epoch 93/100\n",
      "2005/2005 [==============================] - 1s - loss: 0.1667 - acc: 0.9372 - val_loss: 0.6331 - val_acc: 0.7530\n",
      "Epoch 94/100\n",
      "2005/2005 [==============================] - 1s - loss: 0.1771 - acc: 0.9406 - val_loss: 0.6579 - val_acc: 0.7371\n",
      "Epoch 95/100\n",
      "2005/2005 [==============================] - 1s - loss: 0.1704 - acc: 0.9401 - val_loss: 0.6560 - val_acc: 0.7351\n",
      "Epoch 96/100\n",
      "2005/2005 [==============================] - 1s - loss: 0.1618 - acc: 0.9397 - val_loss: 0.6226 - val_acc: 0.7629\n",
      "Epoch 97/100\n",
      "2005/2005 [==============================] - 1s - loss: 0.1608 - acc: 0.9421 - val_loss: 0.6746 - val_acc: 0.7291\n",
      "Epoch 98/100\n",
      "2005/2005 [==============================] - 1s - loss: 0.1596 - acc: 0.9436 - val_loss: 0.6336 - val_acc: 0.7590\n",
      "Epoch 99/100\n",
      "2005/2005 [==============================] - 1s - loss: 0.1615 - acc: 0.9406 - val_loss: 0.6940 - val_acc: 0.7251\n",
      "Epoch 100/100\n",
      "2005/2005 [==============================] - 1s - loss: 0.1705 - acc: 0.9377 - val_loss: 0.6203 - val_acc: 0.7729\n",
      "Test loss: 0.620333035629\n",
      "Test accuracy: 0.772908367484\n"
     ]
    }
   ],
   "source": [
    "reload(polarity)\n",
    "pd = polarity.PD(w2v, acd_)\n",
    "pd.train_pd_keras()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.60756972111553786"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from sklearn.svm import SVC\n",
    "\n",
    "# x_train_flat = x_train.reshape(len(x_train), pd.max_sentence_len * 300)\n",
    "# x_test_flat = x_test.reshape(len(x_test), pd.max_sentence_len * 300)\n",
    "\n",
    "# clf = SVC(kernel='rbf', C=15, random_state=1, probability=True)\n",
    "# clf.fit(x_train_flat, y_train)\n",
    "# predictions = clf.predict_proba(x_test_flat)\n",
    "# pd.calc_accuracy(y_test, predictions, clf.classes_)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "absa",
   "language": "python",
   "name": "absa"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
